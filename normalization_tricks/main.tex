\documentclass{article}

\usepackage{nips_like}
\usepackage{config}


\title{Normalization tricks}
\author{
    Xingdong Zuo\thanks{\today} \\
     \\
    %% \And or \AND
    %% Coauthor \\
    %% Affiliation \\
    %% Address \\
    %% \texttt{email} \\
}

\begin{document}

\maketitle

\begin{definition}
Covariate shift refers to the change of input distribution while the conditional distribution of outputs remains unchanged.
\end{definition}
The behavior of algorithms can change when input distribution changes. For example, each layer in neural networks is forced to continuously adapt to its changing input because the distribution of activations from previous layer changes. 

Normalizing the inputs to be zero mean and unit variance can help to accelerate training (before nonlinearity).

\subsubsection*{Batch normalization}

\begin{algorithm}[H]
\SetAlgoNoLine
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{Mini-batch $\ccB=\set{x_1, \dots, x_m}$; trainable parameters $\gamma, \beta$}
    \Output{$y_i, \forall i=1, \dots, m$}

    \Begin{
        $\mu_{\ccB}\leftarrow \avgsum{i}{1}{m}x_i$ \\
        $\sigma_{\ccB}^2 \leftarrow \avgsum{i}{1}{m}(x_i - \mu_{\ccB})^2 $ \\
        $\hat{x}_i \leftarrow \frac{x_i - \mu_{\ccB}}{\sqrt{\sigma_{\ccB}^2 + \eps}}$ \\
        $y_i \leftarrow \gamma\hat{x}_i + \beta$ 
    }
    \caption{Batch normalization}
    \label{algo:batchnorm}
\end{algorithm}

Limitation of batch normalization
\begin{itemize}
    \item Batch size dependent: small batch size increase variance of estimating statistics. 
    \item Non-trivial to use for RNN: each time step has different statistics. More problematic for varied length sequences. 
\end{itemize}

\subsubsection*{Layer normalization}

Layer normalization normalizes the inputs over the feature dimension instead of batch dimension. 

Let $X\in\bbR^{m\times n}$ be an input data with batch size $m$ and feature size $n$. The estimation of statistics becomes following instead:
\begin{align*}
    \mu_i &\leftarrow \avgsum{j}{1}{n}x_{ij} \\
    \sigma_i^2 &\leftarrow \avgsum{j}{1}{n}(x_{ij} - \mu_i)^2 \\
    \hat{x}_{ij} &\leftarrow \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \eps}}
\end{align*}

Properties
\begin{itemize}
    \item Independent of batch size, useful when batch size is one
    \item Each training example has its own statistics
    \item All neurons in a layer share the same statistics
    \item Useful for RNN with varied sequence length
    \item Not working well for CNN
\end{itemize}

Layer normalization is used in LSTM in the following way:
\begin{align*}
    \begin{pmatrix}
        \bff_t \\
        \bfi_t \\
        \bfo_t \\
        \bfg_t \\
    \end{pmatrix} &= LN(\bfW_h \bfh_{t-1}; \bfgamma_1, \bfbeta_1) + LN(\bfW_x \bfx_t; \bfgamma_2, \bfbeta_2) + \bfb \\
    \bfc_t &= \sigma(\bff_t)\odot\bfc_{t-1} + \sigma(\bfi_t)\odot\tanh(\bfg_t) \\
    \bfh_t &= \sigma(\bfo_t)\odot\tanh(LN(\bfc_t; \bfgamma_3, \bfbeta_3))
\end{align*}
where $LN(\cdot; \bfgamma, \bfbeta)$ is the layer normalization operation with trainable parameters $\bfgamma, \bfbeta$.

\printbibliography

\end{document}